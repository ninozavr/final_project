# -*- coding: utf-8 -*-
"""step

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aCNlEyCjL2fhDBrRLmDjAyOl4PDrI7D9
"""

import requests
from io import StringIO
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.colors as mcolors
import xgboost as xgb
import statsmodels.api as sm
import sklearn as sk
import joblib
import logging
import time
from collections import defaultdict
from scipy.stats import zscore
from scipy.stats import mannwhitneyu, ttest_ind, wilcoxon, ranksums, kruskal
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from sklearn.utils import resample
from statsmodels.formula.api import ols
from scipy.stats import f_oneway
from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import BaggingRegressor, HistGradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score, KFold, GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import r2_score
from sklearn.model_selection import cross_val_score
from xgboost import XGBRegressor



# 1 Data cleaning
def fetch_data(url):
    response = requests.get(url)
    if response.status_code == 200:
        data = pd.read_csv(StringIO(response.text))
        return data
    else:
        print(f"Failed to fetch data. Status code: {response.status_code}")
        return None

link = "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-11-03/ikea.csv"
dataset = fetch_data(link)

# Початковий огляд даних
print(dataset.head())
print(dataset.shape)
print(dataset.info())
print(dataset.columns)
missing_columns = dataset.columns[dataset.isnull().any()].to_list()
print(missing_columns)
print(dataset.isnull().sum())
print(dataset.describe())

unique_data = dataset.drop_duplicates(subset=['item_id'], keep='first').copy()

unique_data['designer'] = unique_data['designer'].apply(lambda x: "/".join(sorted(x.split("/"))))
designer_indices = defaultdict(list)
for i, designer in enumerate(unique_data['designer']):
    sorted_designer = "/".join(sorted(designer.split("/")))
    designer_indices[sorted_designer].append(i)

duplicate_indices = [indices for indices in designer_indices.values() if len(indices) > 1]


designer_duplicates_data = unique_data.iloc[np.concatenate(duplicate_indices)].reset_index(drop=True)

# Перевірка на однакові розміри
same_dims_data = []
for i, row in designer_duplicates_data.iterrows():
    dims = sorted([float(x) if isinstance(x, str) and not pd.isna(x) else x for x in [row['depth'], row['height'], row['width']]])
    row['dims'] = str(tuple(dims))
    row['link'] = re.sub(r'\d+', '', row['link'])
    same_dims_data.append(row)

same_dims_data = pd.DataFrame(same_dims_data)
same_dims_data = same_dims_data[same_dims_data['dims'].duplicated(keep=False)]
grouped_data = same_dims_data.groupby(['name', 'price', 'old_price', 'designer']).filter(lambda x: len(x) > 1)

description_duplicates = grouped_data[grouped_data['short_description'].str.contains(r'\d+')]
description_duplicates = description_duplicates.copy()
description_duplicates['short_description'] = description_duplicates['short_description'].apply(lambda x: "/".join(sorted(re.findall(r'\d+', x))))
description_duplicates = description_duplicates.groupby(['name', 'price', 'old_price', 'designer', 'short_description']).filter(lambda x: len(x) > 1)

description_duplicates = pd.concat([description_duplicates,
                                    grouped_data[grouped_data['short_description'].str.contains(r'\d+') == False]
                                    .groupby(['name', 'price', 'old_price', 'designer'])
                                    .filter(lambda x: len(x) > 1)])


color_names = ['white', 'black', 'red', 'green', 'blue', 'yellow', 'purple', 'pink', 'brown',
               'light-grey', 'dark-grey', 'dark-brown', 'golden', 'brown', 'grey', 'beige', 'brass-colour',
               'dark-red', 'light-brown', 'light-beige', 'orange', 'light-antique', 'steel-colour',
               'anthracite', 'antique', 'turquoise', 'multicoloured-dark', 'multicolour']

color_pattern = r'\b(' + '|'.join(color_names) + r')\b'

for i, row in description_duplicates.iterrows():
    color_matches = re.findall(color_pattern, row['link'])
    if color_matches:
        new_text = '/'.join(sorted(color_matches))
        description_duplicates.loc[i, 'link'] = new_text

same_link = (description_duplicates.groupby(['name', 'price', 'old_price', 'designer', 'short_description', 'link'])
             .filter(lambda x: len(x) > 1))

# Вибірка
same_link_item_ids = same_link['item_id']
data_with_outliers = unique_data[~unique_data['item_id'].isin(same_link_item_ids)]

# Видалення аномалій за ціною
def remove_price_outliers(df):
    q1 = df['price'].quantile(0.25)
    q3 = df['price'].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    return df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]

data_without_price_outliers = remove_price_outliers(data_with_outliers).copy()


data_with_size_outliers = data_without_price_outliers[["price", "depth", "height", "width"]]
data_with_size_outliers = data_with_size_outliers.dropna(subset=["depth", "height", "width"])

numeric_columns = ['price', 'depth', 'height', 'width']


z_scores = zscore(data_with_size_outliers[numeric_columns])
abs_z_scores = np.abs(z_scores)
outlier_indices = np.where(abs_z_scores > 3)[0]
outliers = data_with_size_outliers.iloc[outlier_indices]


size_outliers_index = outliers.index
analysis_data = data_without_price_outliers[~data_without_price_outliers.index.isin(size_outliers_index)].copy()
print(analysis_data)

# 2 Descriptive statistics
# Інформація про набір даних та описова статистика
print(analysis_data.shape)
print(analysis_data.info())
print(analysis_data.describe())

correlation_matrix = analysis_data[['width', 'depth', 'height']].corr()
print(correlation_matrix)
analysis_data['category'].value_counts()
analysis_data[['category', 'sellable_online']].value_counts().reset_index(name='count')
# ТОР 5 найдорожчих товарів
analysis_data.nlargest(5, 'price')
# Показуємо ТОР і LOW 5 товарів по ціні (обрані колонки)
columns_to_print = ['item_id', 'category','name','sellable_online', 'price']
analysis_data_sort = analysis_data.sort_values(by='price', ascending=False)

print("DEB: TOP 5:", analysis_data_sort[columns_to_print].head())
print("DEB: LOW 5:", analysis_data_sort[columns_to_print].tail())
analysis_data.sort_values(by='price', ascending=False)
sns.pairplot(analysis_data[['price', 'depth', 'height', 'width', 'other_colors', 'category']], hue = 'other_colors');

#аналізуємо кількість товарів в кожній категорії, бачимо що найбільше товарі в категорії Bookcases
plt.figure(figsize=(30, 6))
category_order = analysis_data['category'].value_counts().index
sns.countplot(x='category', data=analysis_data, order=category_order, palette='Greens_r', hue='category');
plt.title("Кількість товарів по категоріям")  
plt.xticks(rotation=45, ha='right', fontsize=10)
plt.xlabel('Категорія')
plt.ylabel('Кількість')
plt.legend([])
plt.show()

#heatmap візуалізує кореляційну матрицю між цінами та розмірами (шириною, глибиною та висотою) 
sns.heatmap(analysis_data[['price', 'depth', 'height', 'width']].corr(method= 'spearman'), xticklabels= analysis_data[['price', 'depth', 'height', 'width']].corr().columns,
            yticklabels= analysis_data[['price', 'depth', 'height', 'width']].corr().columns, center= 0, annot= True, cmap='summer');

# Товари-новинки, по яким не було старої ціни
analysis_data_no_old_price = analysis_data[analysis_data['old_price'] == 'No old price']
analysis_data_no_old_price.count()

# Аналізуємо товари, що продаються онлайн, будуємо графік по категоріям цих товарів
analysis_data[analysis_data['sellable_online'] == False].count()
ikea_online = analysis_data[analysis_data['sellable_online'] == False]
ikea_online.describe()
ikea_online
t_x = ikea_online.groupby('category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.xticks(range(int(ax.get_xlim()[0]), int(ax.get_xlim()[1]) + 1))
plt.gca().spines[['top', 'right']].set_visible(False)
plt.show()




# Аналізуємо розподіл цін на товари в різних категоріях та дивимось на наявність викидів. 
# Дивани є найпопулярнішим типом меблів, в середньому найдорожча категорія - Wardrobes, найбільше викидів від середньої ціни ми бачимо у категоріях Sofas & armchairs та Beds

plt.subplots(figsize = (14,7))
category_count = analysis_data['category'].value_counts().index
sns.boxplot(data=analysis_data.sort_values(by='price', ascending=False), x='price', y='category')
plt.show()

#  видаляємо викиди зі стовпця 'price'.
def remove_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

plt.subplots(figsize=(14,7))

analysis_data_no_outliers = remove_outliers(analysis_data, 'price')
category_count = analysis_data_no_outliers['category'].value_counts().index
sns.boxplot(data=analysis_data_no_outliers.sort_values(by='price', ascending=False), x='price', y='category')
plt.show()


 # Описова статистика цін 
price_descriptive_stats = analysis_data["price"].describe()  
price_descriptive_stats_markdown = price_descriptive_stats.to_markdown()  
#print(price_descriptive_stats_markdown)  

# Гістограма цін
plt.hist(analysis_data["price"], bins=10, color='#d2f583')  
plt.title("Аналіз цін")  
plt.xlabel("Ціна")  
plt.ylabel("Частота")  
plt.axvline(analysis_data["price"].mean(), color='b', linestyle='dashed', linewidth=1,  
            label="mean: {:.1f}".format(analysis_data["price"].mean()))
plt.axvline(analysis_data["price"].median(), color='y', linestyle='dashed', linewidth=1,  
            label="median: {:.1f}".format(analysis_data["price"].median()))
plt.axvline(analysis_data["price"].max(), color='r', linestyle='dashed', linewidth=1,  
            label="max: {:.1f}".format(analysis_data["price"].max()))
plt.axvline(analysis_data["price"].min(), color='black', linestyle='dashed', linewidth=1,  
            label="min: {:.1f}".format(analysis_data["price"].min()))
plt.legend()  
plt.show()  

# Описова статистика цін за виробником
price_stats_by_name = analysis_data.groupby("name")['price'].describe()  
price_stats_by_name_markdown = price_stats_by_name.reset_index().to_markdown(index=False)  
median_price_name = analysis_data.groupby("name")["price"].median().reset_index()  
median_price_name = median_price_name.sort_values('price', ascending=False).reset_index(drop=True)
median_price_name = median_price_name.iloc[:21]
median_price_name_markdown = median_price_name.to_markdown(index=False)  

median_price_name['color'] = pd.cut(median_price_name['price'], bins=3, labels=['low', 'medium', 'high'])
colors_list = ["#6B8E23", "#ADFF2F", "#2E8B57"]
fig, ax = plt.subplots(figsize=(14, 7))
cmap = mcolors.LinearSegmentedColormap.from_list("", colors_list)
sns.barplot(x="price", y="name", hue="color", data=median_price_name,
            palette=colors_list,
            orient='horizontal')

sm = plt.cm.ScalarMappable(cmap=cmap)
sm.set_array([])
fig.colorbar(sm, ax=ax)

ax.set_xlim(150, median_price_name['price'].max())

ax.set_xlim(150, median_price_name['price'].max() * 1.1)
plt.yticks(rotation=0, ha='right', fontsize=10)
plt.title("Медіана цін за виробником")
plt.ylabel("")
plt.xlabel("")
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.5)

for i, v in enumerate(median_price_name['price']):
    ax.text(v + 180, i, str(round(v)), ha='center', va='center', color="black", fontsize=11,
            bbox=dict(facecolor='white', edgecolor='w', boxstyle='round,pad=0.05'))

plt.subplots_adjust(left=0.32, bottom=0.1)
plt.gca().invert_yaxis()
plt.show()


# Аналіз кількості товарів по категоріям в залежності від ціни. Ми бачимо основна кількість товарів магазину знаходиться у низькому ціновому сегменті
categories = ['Bookcases & shelving units', 'Chairs', 'Tables & desks', 'Sofas & armchairs', 'Outdoor furniture']

selected_categories = analysis_data['category'].value_counts().nlargest(5).index
data_selected = analysis_data[analysis_data['category'].isin(selected_categories)]

plt.figure(figsize=(14, 10))
plt.title("Розподіл кількості товарів в категорії в залежності від ціни")
plt.xlabel("Кількість")
plt.ylabel("Ціна")
custom_palette = ["#228B22", "#32CD32", "#6B8E23", "#7FFF00", "#00FF00"]
sns.histplot(data=data_selected, y="price", hue="category", multiple="stack", palette=custom_palette)
plt.gca().invert_yaxis()
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

plt.legend(categories, title='Категорія', title_fontsize='14', loc='lower right', fontsize='12')

plt.show()


# Аналіз цін та виробників
print("Unique Manufacturers:", analysis_data["name"].nunique())  
value_counts_by_name = analysis_data.groupby("name")["price"].agg(["count"]).reset_index()  
value_counts_by_name = value_counts_by_name[value_counts_by_name['count'] >= 30] \
    .sort_values('count', ascending=False).reset_index(drop=True)
value_counts_by_name_markdown = value_counts_by_name.to_markdown(index=False)  
print("Manufacturers with >= 30 Products:")  # Виробники з >= 30 товарів
# print(value_counts_by_name_markdown)

plt.figure(figsize=(10, 6))
green_palette = ["#8BC34A", "#AED581", "#C5E1A5"]
gradient_colors = mcolors.LinearSegmentedColormap.from_list("", green_palette)
plt.bar(value_counts_by_name['name'], value_counts_by_name['count'],
        color=gradient_colors(value_counts_by_name['count'] / value_counts_by_name['count'].max()))


plt.grid(axis='y', linestyle='--', alpha=0.5)
for i, v in enumerate(value_counts_by_name['count']):
    plt.text(i, v + 2.5, str(v), ha='center', va='center', color="g",
             bbox=dict(facecolor='white', edgecolor='w', boxstyle='round,pad=0.01'))

plt.xticks(rotation=45, ha='right', fontsize=10)
plt.title("Кількість назв товарів за виробником")
plt.xlabel("Виробник")
plt.ylabel("Кількісь")
plt.grid(axis='y', linestyle='--')
plt.ylim(10, value_counts_by_name['count'].max() * 1.1)
plt.subplots_adjust(bottom=0.15, left=0.1)

plt.show()


# Аналіз цін та дизайнерів

digit_mask = analysis_data['designer'].str.contains('\d')
digit_designer_data = analysis_data[~digit_mask].reset_index(drop=True)

#print(digit_designer_data["designer"].unique())
value_counts_by_designer = digit_designer_data.groupby("designer")["price"].agg(["count"]).reset_index()
value_counts_by_designer = value_counts_by_designer[value_counts_by_designer['count'] >= 30]\
   .sort_values('count', ascending=False).reset_index(drop=True)
#print(value_counts_by_designer)

plt.figure(figsize=(10, 6))
gradient_colors = mcolors.LinearSegmentedColormap.from_list("", ["#8fce00", "#577c03"])  # Встановлюємо зелені кольори
plt.barh(value_counts_by_designer['designer'], value_counts_by_designer['count'],
         color=gradient_colors(value_counts_by_designer['count'] / value_counts_by_designer['count'].max()))

plt.grid(axis='x', linestyle='--', alpha=0.5)
for i, v in enumerate(value_counts_by_designer['count']):
    plt.text(v + 20, i, str(v), ha='center', va='center', color="g",
             bbox=dict(facecolor='white', edgecolor='w', boxstyle='round,pad=0.01'))

plt.yticks(rotation=0, ha='right', fontsize=10)
plt.title("Кількість назв від дизайнерів")
plt.xlabel("")
plt.ylabel("")
plt.grid(axis='x', linestyle='--')
plt.xlim(10, value_counts_by_designer['count'].max() * 1.1)
plt.subplots_adjust(left=0.3, bottom=0.1)

plt.show()

# Перевірка чи більші за розміром продукти роблять одні виробники (і їх ціна вища)
product_category_matrix = pd.pivot_table(analysis_data,
                                         index='category',
                                         columns='name',
                                         values='item_id',
                                         aggfunc='count',
                                         fill_value=0)

# Вибір топ-5 найбільших категорій
top_5_categories = product_category_matrix.sum().nlargest(10).index
product_category_matrix_top5 = product_category_matrix[top_5_categories]

# Створення heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(product_category_matrix_top5, cmap='Greens', annot=True, fmt='d', mask=(product_category_matrix_top5 == 0))
plt.title('Кількість продуктів за категорією та виробником (Топ-10 виробників)')
plt.xlabel('Ім\'я продукту')
plt.ylabel('Категорія')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# Обчислення медіанних цін по категоріям без виробників
median_prices_by_category = analysis_data.groupby('category')['price'].median().sort_values()

plt.figure(figsize=(12,6))
sns.lineplot(x=median_prices_by_category.index, y=median_prices_by_category.values, color ="green", marker='o')
plt.title('Медіанна ціна за категорією')
plt.xlabel('Категорія')
plt.ylabel('Медіанна ціна')
plt.xticks(rotation=45, ha='right')

for x, y in zip(median_prices_by_category.index, median_prices_by_category.values):
    plt.text(x, y, f'{y:.2f}', ha='left', va='bottom', fontsize=6, rotation=0)

plt.grid(True)
plt.tight_layout()
plt.show()


# 3 Hypotheses

#Гіпотеза 1: дизайнерські товари - більш дорогі
#Нульова гіпотеза (H0): немає різниці в ціні в залежності від дизайнера
#Альтернативна гіпотеза (H1): Дизайнерськ меблі дешевші, ніж звичайні

digit_mask = analysis_data['designer'].str.contains('\d')  
designers_data = analysis_data[~digit_mask].reset_index(drop=True)  
designers_counts = designers_data['designer'].value_counts()[::-1]
sum_of_counts = designers_counts.sum()
cumulative_percentage = designers_counts.cumsum() / sum_of_counts
famous_designers = designers_counts[cumulative_percentage > 0.5][::-1]
print(f'Відомі дизайнери, що складають 50% від загальної кількості:')
print(famous_designers)

famous_designers_data = designers_data[designers_data['designer'].isin(famous_designers.index)]
less_known_designers_data = designers_data[~designers_data['designer'].isin(famous_designers.index)]

# t-тест для порівняння середніх цін двох груп
t_statistic, p_value = ttest_ind(famous_designers_data['price'], less_known_designers_data['price'],
                                 equal_var=False)
print(f"T-статистика: {t_statistic:.2f}")
print(f"P-значення: {p_value:.3f}")

famous_designers_mean = np.mean(famous_designers_data['price'])
less_known_designers_mean = np.mean(less_known_designers_data['price'])
difference_means = famous_designers_mean - less_known_designers_mean

if difference_means > 0:
    print(" Альтернативна гіпотеза 1 підтверджується: меблі відомих дизайнерів дорожчі за меблі менш відомих дизайнерів. ")
else:
    print(" Альтернативна гіпотеза 2 підтверджується: меблі менш відомих дизайнерів дорожчі за меблі відомих дизайнерів. ")

# Висновок на основі t-тесту
'''Альтернативна гіпотеза підтверджується для даної гіпотези. P-значення 0.000 вказує на те,
що ймовірність спостереження такої великої різниці середніх значень між двома групами є дуже малою.
Інакше кажучи, нульову гіпотезу про відсутність різниці між середніми цінами продуктів обох груп можна відкинути з високим рівнем впевненості.'''

# Mann-Whitney U test
statistic, p_value = mannwhitneyu(famous_designers_data['price'], less_known_designers_data['price'])

print(f"Статистика: {statistic:.2f}")
print(f"P-значення: {p_value:.3f}")

# Висновок на основі тесту Манна-Уітні
'''Якщо тест Манна-Уітні дає p-значення 0.000, це означає, що тест виявив значну різницю в середній ціні меблів між відомими і менш відомими дизайнерами.
Оскільки p-значення менше за типове значення значущості 0.05, ми можемо відкинути нульову гіпотезу і зробити висновок,
що є статистично значуща різниця в медіанній ціні меблів між#двома групами.Отже, тест Манна-Уітні та різниця в середніх значеннях "

# ANOVA test
f_statistic, p_value_anova = f_oneway(famous_designers_data['price'], less_known_designers_data['price'])

print("F-статистика: {:.2f}".format(f_statistic))
print("P-значення (ANOVA): {:.4f}".format(p_value_anova))

alpha = 0.05
if p_value_anova < alpha:
    print("Відхиляємо нульову гіпотезу. Існує статистично значуща різниця в середній ціні меблів між відомими та менш відомими дизайнерами.")
else:
    print("Не можемо відхилити нульову гіпотезу. Немає статистично значущої різниці в середній ціні меблів між відомими та менш відомими дизайнерами.")


# Shuffle test
famous_prices_values = famous_designers_data['price'].values
less_known_prices_values = less_known_designers_data['price'].values

observed_diff = np.mean(famous_prices_values) - np.mean(less_known_prices_values)

concatenated_prices = np.concatenate([famous_prices_values, less_known_prices_values])
np.random.shuffle(concatenated_prices)

shuffled_famous_prices = concatenated_prices[:len(famous_prices_values)]
shuffled_less_known_prices = concatenated_prices[len(famous_prices_values):]

shuffled_diff = np.mean(shuffled_famous_prices) - np.mean(shuffled_less_known_prices)

n_permutations = 1000
null_distribution = np.zeros(n_permutations)
for i in range(n_permutations):
    np.random.shuffle(concatenated_prices)
    shuffled_famous_prices = concatenated_prices[:len(famous_prices_values)]
    shuffled_less_known_prices = concatenated_prices[len(famous_prices_values):]
    null_distribution[i] = np.mean(shuffled_famous_prices) - np.mean(shuffled_less_known_prices)

p_value = np.mean(np.abs(null_distribution) >= np.abs(observed_diff))

print(f"Спостережувана різниця середніх значень: {observed_diff:.2f}")
print(f"P-значення: {p_value:.4f}")

'''На основі спостережуваної різниці середніх значень -133.75 та p-значення 0.0000,
ми можемо відкинути нульову гіпотезу та зробити висновок, що є значна різниця в цінах на меблі між популярними та менш відомими дизайнерами.
Крім того, альтернативна гіпотеза 2 підтверджується, згідно з якою меблі, створені менш відомими дизайнерами, дорожчі за меблі, створені популярними дизайнерами.'''

# Tukey's HSD test
famous_prices = famous_designers_data['price']
less_known_prices = less_known_designers_data['price']

tukey_results = pairwise_tukeyhsd(np.concatenate([famous_prices, less_known_prices]),
                                  np.concatenate([['famous'] * len(famous_prices), ['less-known'] * len(less_known_prices)]))

print(tukey_results.summary())

'''Результат показує, що середня різниця між групою "відомі" та групою "менш відомі" становить 133.7527 з p-значенням 0.0.
Це означає, що середня ціна продуктів, розроблених відомими дизайнерами, значно нижча за середню ціну продуктів, розроблених менш відомими дизайнерами.
Нижня та верхня межі довірчого інтервалу становлять 76.1586 та 191.3468 відповідно.
Оскільки довірчий інтервал не містить нуля, ми можемо відкинути нульову гіпотезу та зробити висновок,
що різниця в середніх цінах між двома групами є статистично значущою на рівні значущості 0.05.'''




#Гіпотеза 2:
#Гіпотеза: Наявність товару онлайн може впливати на його ціну.
#Нульова гіпотеза (H0): Факт продажу товару онлайн не впливає на його ціну.
#Альтернативна гіпотеза (H1): Факт продажу товару онлайн впливає на його ціну.

online_sales = analysis_data[analysis_data['sellable_online']]['price']
offline_sales = analysis_data[~analysis_data['sellable_online']]['price']
t_statistic, p_value = ttest_ind(online_sales, offline_sales, equal_var=False)

print(f"T-statistic: {t_statistic:.2f}")
print(f"P-value: {p_value:.3f}")

alpha = 0.05
if p_value < alpha:
    print("Відхиляємо нульову гіпотезу. Наявність можливості продажу товару онлайн впливає на його ціну.")
else:
    print("Не можемо відхилити нульову гіпотезу. Наявність можливості продажу товару онлайн не впливає на його ціну.")
# немає статистично значущих доказів того, що наявність можливості продажу товару онлайн впливає на його ціну.

# Kruskal-Wallis test
statistic, p_value = kruskal(online_sales, offline_sales)

print("Test statistic (Kruskal-Wallis): {:.2f}".format(statistic))
print("P-value (Kruskal-Wallis): {:.4f}".format(p_value))

if p_value < alpha:
    print("Відхиляємо нульову гіпотезу. Наявність можливості продажу товару онлайн впливає на його ціну.")
else:
    print("Не можемо відхилити нульову гіпотезу. Наявність можливості продажу товару онлайн не впливає на його ціну.")
'''p-значення (0.0525) більше за рівень значущості (0.05), що означає, що немає достатніх доказів на користь того,
що наявність товару онлайн впливає на його ціну. Таким чином, ми не можемо відхилити нульову гіпотезу і приймаємо,
що немає статистично значущої різниці в ціні між товарами, які продаються онлайн і товарами, які не продаються онлайн.'''

# Bootstrap Test
def diff_means(x, y):
    return np.mean(x) - np.mean(y)

n_iterations = 1000
differences = []
for i in range(n_iterations):
    sample_a = resample(online_sales, replace=True, n_samples=len(online_sales))
    sample_b = resample(offline_sales, replace=True, n_samples=len(offline_sales))
    difference = diff_means(sample_a, sample_b)
    differences.append(difference)

alpha = 0.05
lower = np.percentile(differences, alpha/2*100)
upper = np.percentile(differences, (1-alpha/2)*100)

if lower <= 0 <= upper:
    print("Немає суттєвої різниці в середніх цінах.")
else:
    print("Є суттєва різниця в середніх цінах.")
#У нашому випадку ми перевіряємо, чи є суттєва різниця в середніх цінах між товаром, який продається онлайн, та тим, який не продається.
#Результати вказують на те, що різниці немає і не можемо відхилити нульову гіпотезу

# Shuffle test
observed_diff = np.mean(online_sales) - np.mean(offline_sales)

n_iterations = 1000
differences = []
combined_data = pd.concat([online_sales, offline_sales])

for i in range(n_iterations):
    np.random.shuffle(combined_data.values)
    sample_a = combined_data[:len(online_sales)]
    sample_b = combined_data[len(online_sales):]
    difference = np.mean(sample_a) - np.mean(sample_b)
    differences.append(difference)

p_value = (np.abs(differences) >= np.abs(observed_diff)).mean()

print("Observed difference in means: {:.2f}".format(observed_diff))
print("P-value: {:.4f}".format(p_value))

if p_value < 0.05:
    print("Є суттєва різниця в середніх цінах між товарами, які продаються онлайн "
          "і товарами, які не продаються онлайн, (p = {:.3f}).".format(p_value))
else:
    print("Немає суттєвої різниці в середніх цінах між товарами, які продаються онлайн "
          "і товарами, які не продаються онлайн, (p = {:.3f}).".format(p_value))

# p-значення дорівнює 0.168, що більше за рівень значущості 0.05.
# Отже, ми не маємо достатніх доказів для відхилення нульової гіпотези про відсутність різниці в середніх цінах між товарами, які продаються онлайн та товарами, які не продаються онлайн.




#4 Mashine learning
analysis_dataset = pd.DataFrame(analysis_data.values, columns=analysis_data.columns) #створила копію, щоб виключити непотрібні колонки і проводити навчання на копіі

def cleanDesigners(value):
  if 'IKEA of Sweden/' in value:
      value = value.split('/')[1]
  elif '/IKEA of Sweden' in value:
      value = value.split('/')[0]
  elif 'IKEA of Sweden' in value:
      value = 'Unknown'
  elif re.match(r'^\d+\.\d+\.\d+', value):
      value = 'Unknown'

  return value

analysis_dataset['designer_clean'] = analysis_dataset['designer'].apply(cleanDesigners)
analysis_dataset['designer_clean'].unique()

def newColumnDesigner(value):
  if value == 'Unknown':
      return 0
  else:
      return 1
analysis_dataset['special_designed'] = analysis_dataset['designer'].apply(newColumnDesigner)

def newColumnColours(value):
  if value == 'Yes':
      return 1
  else:
      return 0
analysis_dataset['other_colors_1'] = analysis_dataset['other_colors'].apply(newColumnColours)
analysis_dataset.info()


# Обчислення медіан для рядків 'depth', 'height', 'width' за групами товарів
median_d = analysis_dataset.groupby(['category'])['depth'].median()
median_h = analysis_dataset.groupby(['category'])['height'].median()
median_w = analysis_dataset.groupby(['category'])['width'].median()

#  Медіани 'price' для кожної категорії товарів та по дизайнерам після внесення того як ми їх почистили
median_price = analysis_dataset.groupby(['category'])['price'].median()
median_price_old = analysis_dataset.groupby(['category'])['price'].median()
median_dsgn =  analysis_dataset.groupby(['designer'])['price'].median()

# Заповнення пропущених значень та створення нових колонок
analysis_dataset = analysis_dataset.set_index(['category'])
analysis_dataset['depth_1'] = analysis_dataset['depth'].fillna(median_d)
analysis_dataset['height_1'] = analysis_dataset['height'].fillna(median_h)
analysis_dataset['width_1'] = analysis_dataset['width'].fillna(median_w)
analysis_dataset['category_median_price'] = median_price

analysis_dataset = analysis_dataset.reset_index()
analysis_dataset.head()
exclude_cols = ['item_id', 'old_price', 'price']
if 'index' in analysis_dataset.columns:
    analysis_dataset = analysis_dataset.drop('index', axis=1)
print(analysis_dataset.info())
print(analysis_dataset.columns)

X = analysis_dataset[['depth', 'width', 'height', 'category', 'designer_clean', 'other_colors']]
Y = analysis_dataset['price']
X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)

numeric_transf = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('impute', SimpleImputer(strategy='median'))
])

categorical_transf = Pipeline(steps=[
    ('impute', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])


col_prepr = ColumnTransformer(transformers=[
    ('numeric', numeric_transf, ['depth', 'width', 'height']),
    ('categorical', categorical_transf, ['category', 'designer_clean', 'other_colors'])
])

dtr = Pipeline(steps=[
    ('col_prep', col_prepr),
    ('dtr', DecisionTreeRegressor(max_depth=10, random_state=42))
])

dtr.fit(X_train, Y_train)
dtr_predict = dtr.predict(X_test)

# Метрики опису моделі
print('R^2 : {:.5f}'.format(dtr.score(X_test, Y_test)))
print('MAE : {:.5f}'.format(sk.metrics.mean_absolute_error(dtr_predict, Y_test)))
print('MSE : {:.5f}'.format(np.sqrt(sk.metrics.mean_squared_error(dtr_predict, Y_test))))
print('MAPE : {:.5f}'.format(np.mean(np.abs((Y_test - dtr_predict) / Y_test)) * 100))
print('Explained Variance Score : {:.5f}'.format(sk.metrics.explained_variance_score(Y_test, dtr_predict)))
print('Median Absolute Error : {:.5f}'.format(sk.metrics.median_absolute_error(Y_test, dtr_predict)))
print('Mean Absolute Error : {:.5f}'.format(sk.metrics.mean_absolute_error(Y_test, dtr_predict)))


# Boxplot of price
plt.figure(figsize=(10, 6))
sns.boxplot(data=analysis_dataset, x='price')
plt.title('Boxplot of Price')
plt.xlabel('Price')
plt.show()

# Scatter plot of price vs. width_1
plt.figure(figsize=(10, 6))
sns.scatterplot(data=analysis_dataset, x='width_1', y='price')
plt.title('Scatter Plot of Price vs. Width')
plt.xlabel('Width')
plt.ylabel('Price')
plt.show()

dtr



# Оцінюємо декілька регресійних моделей та визначаємо, яка з них є найкращою за допомогою певних метрик якості.

categorical_features = ['sellable_online', 'special_designed', 'other_colors_1']
numeric_features = ['width_1', 'depth_1', 'height_1']

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])

def getBestRegressor(X, Y):
    X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)

    models = [
        sk.linear_model.LinearRegression(),
        sk.linear_model.LassoCV(),
        sk.linear_model.RidgeCV(),
        sk.svm.SVR(kernel='linear'),
        sk.neighbors.KNeighborsRegressor(n_neighbors=16),
        sk.tree.DecisionTreeRegressor(max_depth=10, random_state=42),
        RandomForestRegressor(random_state=42),
        GradientBoostingRegressor(),
        XGBRegressor(random_state=42)
    ]

    TestModels = pd.DataFrame()
    res = {}

    for model in models:
        m = str(model)
        tmp = {'Model': m[:m.index('(')]}
        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
        pipeline.fit(X_train, Y_train)
        tmp['R^2'] = pipeline.score(X_test, Y_test)
        tmp['MAE'] = sk.metrics.mean_absolute_error(pipeline.predict(X_test), Y_test)
        tmp['RMSE'] = np.sqrt(sk.metrics.mean_squared_error(pipeline.predict(X_test), Y_test))

        TestModels = pd.concat([TestModels, pd.DataFrame([tmp])])

    TestModels.set_index('Model', inplace=True)
    res['model'] = TestModels
    res['X_train'] = X_train
    res['Y_train'] = Y_train
    res['X_test'] = X_test
    res['Y_test'] = Y_test
    return res

X = analysis_dataset[['sellable_online', 'width_1', 'depth_1', 'height_1', 'special_designed', 'other_colors_1']]
Y = analysis_dataset['price']

test1 = getBestRegressor(X, Y)
print(test1['model'].sort_values(by='R^2', ascending=False))

# Візуалізація boxplot of price
plt.figure(figsize=(10, 6))
sns.boxplot(data=analysis_dataset, x='price', color='green')
plt.title('Boxplot of Price')
plt.xlabel('Price')
plt.show()

# Візуалізація scatter plot of price vs. width_1
plt.figure(figsize=(10, 6))
sns.scatterplot(data=analysis_dataset, x='width_1', y='price', color='green')
plt.title('Scatter Plot of Price vs. Width')
plt.xlabel('Width')
plt.ylabel('Price')
plt.show()

# Візуалізація bar plot of R^2 values for different models
plt.figure(figsize=(10, 6))
test1['model']['R^2'] = test1['model']['R^2'].astype(float)  # Convert R^2 values to float
test1['model']['R^2'].sort_values().plot(kind='barh', color='green')
plt.title('R^2 Values for Different Models')
plt.xlabel('R^2')
plt.ylabel('Model')
plt.show()

# Візуалізація порівняння RMSE різних моделей
plt.figure(figsize=(10, 6))
test1['model']['RMSE'] = test1['model']['RMSE'].astype(float)  # Convert RMSE values to float
test1['model']['RMSE'].sort_values().plot(kind='barh', color='green')
plt.title('RMSE Values for Different Models')
plt.xlabel('RMSE')
plt.ylabel('Model')
plt.show()


"""
Висновок
RandomForestRegressor є найкращою моделлю для прогнозування ціни, оскільки має найвищий R^2 і найменші значення MAE та RMSE. 
XGBRegressor також показав дуже хороші результати і може бути використаний як альтернативна модель. Інші моделі, такі як GradientBoostingRegressor, 
також можуть бути корисними, але вони поступаються першими двома моделям. Моделі LinearRegression, RidgeCV, LassoCV та SVR мають значно нижчі показники точності та більші похибки.
Ці результати представляють оцінку кількох різних регресійних моделей на основі метрик ефективності моделі. Ось що означають ці метрики:
R² (R-квадрат): Це коефіцієнт детермінації, який вимірює, наскільки добре модель пояснює варіацію вихідної змінної. Він приймає значення від 0 до 1, де 1 означає ідеальне узгодження моделі з даними, а 0 означає жодного узгодження. Чим вище значення R², тим краще модель пояснює варіацію вихідної змінної.
MAE (Mean Absolute Error): Це середня абсолютна помилка між прогнозованими і спостережуваними значеннями. Вона вимірює середнє значення різниці між прогнозованими і спостережуваними значеннями. Менші значення MAE вказують на кращу точність моделі.
RMSE (Root Mean Squared Error): Це квадратний корінь з середньоквадратичної помилки між прогнозованими і спостережуваними значеннями. Вона вимірює рівень відхилення прогнозування моделі від фактичних даних. Чим менше значення RMSE, тим краще модель.
На основі цих метрик можна зробити висновок про те, яка модель найкраще підходить для конкретного набору даних. Наприклад, у вас вище значення R² та найменші значення MAE і RMSE для моделі RandomForestRegressor, що свідчить про її найвищу ефективність серед усіх оцінених моделей.
"""



X = analysis_dataset[['sellable_online', 'width_1', 'depth_1', 'height_1', 'special_designed', 'other_colors_1']]
Y = analysis_dataset['price']

X_train, X_test, Y_train, Y_test = sk.model_selection.train_test_split(X, Y, test_size=0.2, random_state=42)

rf_model = RandomForestRegressor(n_estimators=100, max_depth=None, max_features='auto', random_state=42)
rf_model.fit(X_train, Y_train)

cv_scores = cross_val_score(rf_model, X_train, Y_train, cv=5, scoring='r2')

print('Cross-Validation Scores:', cv_scores)
print('Mean CV Score:', np.mean(cv_scores))

print('Best Estimator :', rf_model)
print('Best Score     :', rf_model.score(X_train, Y_train))
print('Best Params    :', {'n_estimators': 100, 'max_depth': None, 'max_features': 'auto'})
print('')
print('R^2                      : {:.5f}'.format(sk.metrics.r2_score(Y_test, rf_model.predict(X_test))))
print('')
print('Feature importance:')
print('--------------------------------')
for feat, importance in zip(X_train.columns, rf_model.feature_importances_):
    print('{:.5f}    {f}'.format(importance, f=feat))



"""Результати оцінки моделі RandomForestRegressor:

Cross-Validation Scores: [0.6038533, 0.68899571, 0.69340457, 0.65968205, 0.67166642]

Середнє значення R² для крос-валідації: приблизно 0.664
Найкращий Estimator: RandomForestRegressor з параметрами:

max_features='auto'
n_estimators=100
max_depth=None
random_state=42
Найкращий Score: 0.856

Це значення R² на тренувальному наборі даних
R² на тестовому наборі: 0.652

Це значення коефіцієнта детермінації, що вказує на узгодженість моделі з тестовими даними.
Важливість ознак:

sellable_online: 0.00114
width_1: 0.55081
depth_1: 0.19618
height_1: 0.22264
special_designed: 0.0
other_colors_1: 0.02923
Висновок: Модель RandomForestRegressor показала досить стабільну продуктивність на тренувальному і тестовому наборах даних, 
з R² близько 0.652 на тестовому наборі. Найважливішими ознаками для прогнозування ціни є ширина, глибина і висота, 
які мають великий вплив на прогнозовану ціну, 
з іншими ознаками, такими як продаж в інтернеті та інші кольори, які мають менший вплив.
"""



#label_encoder = LabelEncoder()
X['sellable_online'] = label_encoder.fit_transform(X['sellable_online'])

# Оцінка крос-валідації за допомогою XGBoostRegressor
xgb_reg = XGBRegressor()
rmse_scores_xgb_regression = np.sqrt(-cross_val_score(xgb_reg, X, Y, scoring="neg_mean_squared_error", cv=10))
rmse_mean_xgb_regression = rmse_scores_xgb_regression.mean()
print("Mean RMSE for XGBoost Regression:", rmse_mean_xgb_regression)
print("RMSE scores for each fold:", rmse_scores_xgb_regression)

# Побудова графіку
fig, ax = plt.subplots(figsize=(20, 8))

ax.barh(range(len(rmse_scores_xgb_regression)), rmse_scores_xgb_regression, color='green')
ax.set_yticks(range(len(rmse_scores_xgb_regression)))
ax.set_yticklabels(['Fold {}'.format(i+1) for i in range(len(rmse_scores_xgb_regression))])
plt.axvline(rmse_mean_xgb_regression, color='y', linestyle='dashed', linewidth=2.5,
            label=f"RMSE: {round(rmse_mean_xgb_regression)}")
for i, v in enumerate(rmse_scores_xgb_regression):
    plt.text(v+7, i, f"{round(v)}", ha='left', va='center', color="black", fontsize=12,
             bbox=dict(facecolor='orange', edgecolor='pink', boxstyle='round4,pad=0.4'))
ax.set_xlim(0, rmse_mean_xgb_regression.max() * 1.8)
plt.xlabel('RMSE')
plt.ylabel('Fold')
plt.title('Cross-validation results of XGBoost Regression')
plt.legend(loc='upper right', fontsize=13)
plt.gca().grid(False)

plt.show()

"""
Корінь середньоквадратичної помилки (RMSE)
RMSE (Root Mean Squared Error) є метрикою, яка вимірює середню величину помилки в прогнозах моделі. Вона обчислюється як квадратний корінь з середнього квадрата різниць між прогнозованими значеннями та фактичними значеннями. Нижче значення RMSE означає кращу якість прогнозу.
Результати крос-валідації
Mean RMSE for XGBoost Regression: 483.74333893931527

Це середнє значення RMSE по всіх фолдах крос-валідації. У цьому випадку середнє значення RMSE становить приблизно 483.74. 
Це означає, що в середньому модель робить помилку приблизно на 483.74 в прогнозах цін.
RMSE scores for each fold: [623.83155586 263.62408764 338.46719579 368.64269927 516.72135435 377.79605118 630.6847613 449.75718542 580.6793121 687.22918647]

Це RMSE для кожного з 10 фолдів крос-валідації. Вони показують, як модель працювала на кожному з розділів даних.
Значення RMSE варіюються від 263.62 до 687.23, що свідчить про те, що продуктивність моделі може коливатися залежно від того, 
на якому наборі даних вона тестується. Наприклад:
Найнижче RMSE (263.62) вказує на те, що на цьому фолді модель прогнозувала ціни з найменшою помилкою.
Найвище RMSE (687.23) вказує на те, що на цьому фолді модель мала найбільшу помилку в прогнозах.
Висновки
Середнє значення RMSE показує, що модель в середньому робить помилку приблизно на 483.74 в прогнозах цін. 
Це дає загальне уявлення про те, наскільки добре модель прогнозує ціни.
Коливання RMSE між фолдами свідчать про варіабельність продуктивності моделі на різних частинах даних. 
Це може бути пов'язано з тим, що різні фолди можуть містити різні типи даних, які модель обробляє по-різному.
"""





# Збереження моделі у файл
joblib.dump(forest_grid.best_estimator_, 'random_forest_model.pkl')

""" #Завантаження моделі з файлу
loaded_model = joblib.load('random_forest_model.pkl')

# Використання завантаженої моделі для прогнозування
predicted_prices = loaded_model.predict(new_data)
print(predicted_prices)"""

start = datetime.datetime.now()
parameters = {'n_estimators': [10, 20, 30, 50],
              'max_depth': [None, 5, 10, 20],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4],
              'max_features': ['auto', 'sqrt', 'log2']
              }

rf = RandomForestRegressor()

grid_search = GridSearchCV(rf, parameters, cv = 5, n_jobs= -1 )
grid_search.fit(X_train, Y_train)

end = datetime.datetime.now()
print(end - start )
print("Best Parameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)

logging.basicConfig(filename = 'machine_learning_traning.log', filemode= 'w', level= logging.INFO, format= '%(asctime)s - %(levelname)s - %(message)s')
def start_pipeline(pipeline, X_train, Y_train, X_test, Y_test) -> None:
    start_time = time.time()
    logging.info('Start model traning')
    pipeline.fit(X_train, Y_train)
    dtr_predict = pipeline.predict(X_test)
    logging.info(f'Model: {pipeline} Accuracy: {str(pipeline.score(X_test, Y_test))}')









